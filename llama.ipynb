{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bdxEUQYDYI8g",
        "outputId": "659a3062-6d83-4aef-97e4-b33f193865a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting llama-parse\n",
            "  Downloading llama_parse-0.6.51-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting llama-cloud-services>=0.6.51 (from llama-parse)\n",
            "  Downloading llama_cloud_services-0.6.51-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.7 in /usr/local/lib/python3.11/dist-packages (from llama-cloud-services>=0.6.51->llama-parse) (8.2.1)\n",
            "Collecting llama-cloud==0.1.34 (from llama-cloud-services>=0.6.51->llama-parse)\n",
            "  Downloading llama_cloud-0.1.34-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting llama-index-core>=0.12.0 (from llama-cloud-services>=0.6.51->llama-parse)\n",
            "  Downloading llama_index_core-0.12.52.post1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: platformdirs<5.0.0,>=4.3.7 in /usr/local/lib/python3.11/dist-packages (from llama-cloud-services>=0.6.51->llama-parse) (4.3.8)\n",
            "Requirement already satisfied: pydantic!=2.10,>=2.8 in /usr/local/lib/python3.11/dist-packages (from llama-cloud-services>=0.6.51->llama-parse) (2.11.7)\n",
            "Collecting python-dotenv<2.0.0,>=1.0.1 (from llama-cloud-services>=0.6.51->llama-parse)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: tenacity<10.0,>=8.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cloud-services>=0.6.51->llama-parse) (8.5.0)\n",
            "Requirement already satisfied: certifi>=2024.7.4 in /usr/local/lib/python3.11/dist-packages (from llama-cloud==0.1.34->llama-cloud-services>=0.6.51->llama-parse) (2025.7.14)\n",
            "Requirement already satisfied: httpx>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cloud==0.1.34->llama-cloud-services>=0.6.51->llama-parse) (0.28.1)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.12.0->llama-cloud-services>=0.6.51->llama-parse) (3.12.14)\n",
            "Collecting aiosqlite (from llama-index-core>=0.12.0->llama-cloud-services>=0.6.51->llama-parse)\n",
            "  Downloading aiosqlite-0.21.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting banks<3,>=2.2.0 (from llama-index-core>=0.12.0->llama-cloud-services>=0.6.51->llama-parse)\n",
            "  Downloading banks-2.2.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting dataclasses-json (from llama-index-core>=0.12.0->llama-cloud-services>=0.6.51->llama-parse)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting deprecated>=1.2.9.3 (from llama-index-core>=0.12.0->llama-cloud-services>=0.6.51->llama-parse)\n",
            "  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting dirtyjson<2,>=1.0.8 (from llama-index-core>=0.12.0->llama-cloud-services>=0.6.51->llama-parse)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting filetype<2,>=1.2.0 (from llama-index-core>=0.12.0->llama-cloud-services>=0.6.51->llama-parse)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.12.0->llama-cloud-services>=0.6.51->llama-parse) (2025.3.0)\n",
            "Collecting llama-index-workflows<2,>=1.0.1 (from llama-index-core>=0.12.0->llama-cloud-services>=0.6.51->llama-parse)\n",
            "  Downloading llama_index_workflows-1.2.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.12.0->llama-cloud-services>=0.6.51->llama-parse) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.12.0->llama-cloud-services>=0.6.51->llama-parse) (3.5)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.12.0->llama-cloud-services>=0.6.51->llama-parse) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.12.0->llama-cloud-services>=0.6.51->llama-parse) (2.0.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.12.0->llama-cloud-services>=0.6.51->llama-parse) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.12.0->llama-cloud-services>=0.6.51->llama-parse) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.12.0->llama-cloud-services>=0.6.51->llama-parse) (2.32.3)\n",
            "Collecting setuptools>=80.9.0 (from llama-index-core>=0.12.0->llama-cloud-services>=0.6.51->llama-parse)\n",
            "  Downloading setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core>=0.12.0->llama-cloud-services>=0.6.51->llama-parse) (2.0.41)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.12.0->llama-cloud-services>=0.6.51->llama-parse) (0.9.0)\n",
            "Requirement already satisfied: tqdm<5,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.12.0->llama-cloud-services>=0.6.51->llama-parse) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.12.0->llama-cloud-services>=0.6.51->llama-parse) (4.14.1)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core>=0.12.0->llama-cloud-services>=0.6.51->llama-parse)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.12.0->llama-cloud-services>=0.6.51->llama-parse) (1.17.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=2.10,>=2.8->llama-cloud-services>=0.6.51->llama-parse) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=2.10,>=2.8->llama-cloud-services>=0.6.51->llama-parse) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=2.10,>=2.8->llama-cloud-services>=0.6.51->llama-parse) (0.4.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core>=0.12.0->llama-cloud-services>=0.6.51->llama-parse) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core>=0.12.0->llama-cloud-services>=0.6.51->llama-parse) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core>=0.12.0->llama-cloud-services>=0.6.51->llama-parse) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core>=0.12.0->llama-cloud-services>=0.6.51->llama-parse) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core>=0.12.0->llama-cloud-services>=0.6.51->llama-parse) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core>=0.12.0->llama-cloud-services>=0.6.51->llama-parse) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core>=0.12.0->llama-cloud-services>=0.6.51->llama-parse) (1.20.1)\n",
            "Collecting griffe (from banks<3,>=2.2.0->llama-index-core>=0.12.0->llama-cloud-services>=0.6.51->llama-parse)\n",
            "  Downloading griffe-1.8.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.2.0->llama-index-core>=0.12.0->llama-cloud-services>=0.6.51->llama-parse) (3.1.6)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->llama-cloud==0.1.34->llama-cloud-services>=0.6.51->llama-parse) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->llama-cloud==0.1.34->llama-cloud-services>=0.6.51->llama-parse) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->llama-cloud==0.1.34->llama-cloud-services>=0.6.51->llama-parse) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.20.0->llama-cloud==0.1.34->llama-cloud-services>=0.6.51->llama-parse) (0.16.0)\n",
            "Collecting llama-index-instrumentation>=0.1.0 (from llama-index-workflows<2,>=1.0.1->llama-index-core>=0.12.0->llama-cloud-services>=0.6.51->llama-parse)\n",
            "  Downloading llama_index_instrumentation-0.3.0-py3-none-any.whl.metadata (252 bytes)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core>=0.12.0->llama-cloud-services>=0.6.51->llama-parse) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core>=0.12.0->llama-cloud-services>=0.6.51->llama-parse) (2024.11.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core>=0.12.0->llama-cloud-services>=0.6.51->llama-parse) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core>=0.12.0->llama-cloud-services>=0.6.51->llama-parse) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core>=0.12.0->llama-cloud-services>=0.6.51->llama-parse) (3.2.3)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core>=0.12.0->llama-cloud-services>=0.6.51->llama-parse)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core>=0.12.0->llama-cloud-services>=0.6.51->llama-parse)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core>=0.12.0->llama-cloud-services>=0.6.51->llama-parse) (25.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.20.0->llama-cloud==0.1.34->llama-cloud-services>=0.6.51->llama-parse) (1.3.1)\n",
            "Collecting colorama>=0.4 (from griffe->banks<3,>=2.2.0->llama-index-core>=0.12.0->llama-cloud-services>=0.6.51->llama-parse)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->banks<3,>=2.2.0->llama-index-core>=0.12.0->llama-cloud-services>=0.6.51->llama-parse) (3.0.2)\n",
            "Downloading llama_parse-0.6.51-py3-none-any.whl (4.9 kB)\n",
            "Downloading llama_cloud_services-0.6.51-py3-none-any.whl (48 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_cloud-0.1.34-py3-none-any.whl (289 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_core-0.12.52.post1-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading banks-2.2.0-py3-none-any.whl (29 kB)\n",
            "Downloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
            "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading llama_index_workflows-1.2.0-py3-none-any.whl (37 kB)\n",
            "Downloading setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading aiosqlite-0.21.0-py3-none-any.whl (15 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading llama_index_instrumentation-0.3.0-py3-none-any.whl (15 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Downloading griffe-1.8.0-py3-none-any.whl (132 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.5/132.5 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Installing collected packages: filetype, dirtyjson, setuptools, python-dotenv, mypy-extensions, marshmallow, deprecated, colorama, aiosqlite, typing-inspect, griffe, llama-index-instrumentation, llama-cloud, dataclasses-json, banks, llama-index-workflows, llama-index-core, llama-cloud-services, llama-parse\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 75.2.0\n",
            "    Uninstalling setuptools-75.2.0:\n",
            "      Successfully uninstalled setuptools-75.2.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiosqlite-0.21.0 banks-2.2.0 colorama-0.4.6 dataclasses-json-0.6.7 deprecated-1.2.18 dirtyjson-1.0.8 filetype-1.2.0 griffe-1.8.0 llama-cloud-0.1.34 llama-cloud-services-0.6.51 llama-index-core-0.12.52.post1 llama-index-instrumentation-0.3.0 llama-index-workflows-1.2.0 llama-parse-0.6.51 marshmallow-3.26.1 mypy-extensions-1.1.0 python-dotenv-1.1.1 setuptools-80.9.0 typing-inspect-0.9.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "badfaf32d2a14ffeaf3bf7e4812f9c90",
              "pip_warning": {
                "packages": [
                  "_distutils_hack"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install llama-parse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bXeHU4z1YPNl"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "import os\n",
        "LLAMA_CLOUD_API_KEY=\"llx-cMJE4Wcp5scmIbiGrdT4DJIqzUng5UZuGa0fqho3Mm2E0MP1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "W6L-N-HjZSfd"
      },
      "outputs": [],
      "source": [
        "from llama_parse import LlamaParse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "aZEtCgzF5Lh-"
      },
      "outputs": [],
      "source": [
        "  parsinginstruct = \"\"\"\n",
        "You are an intelligent document parser designed to extract a structured outline from research papers, reports, whitepapers, policy documents, and technical PDFs.\n",
        "\n",
        "Your goal is to produce a clean, hierarchical representation of the document’s headings in the following JSON format:\n",
        "\n",
        "{\n",
        "  \"title\": \"<document title>\",\n",
        "  \"outline\": [\n",
        "    {\n",
        "      \"level\": \"H1\",\n",
        "      \"text\": \"<heading text>\",\n",
        "      \"page\": <page number>\n",
        "    },\n",
        "    ...\n",
        "  ]\n",
        "}\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "1. Title Extraction:\n",
        "   - Extract the document title from the first page or cover.This is the title for the entire document.It should appear only once in the json file for the entire pdf.\n",
        "   - It is usually the largest, most centered, and prominent text.\n",
        "   - It may span multiple lines.\n",
        "   - Do not extract logos or headers.\n",
        "   1. The **document title** (only once for the entire document)\n",
        "\n",
        "2. Outline Extraction:\n",
        "   - Only extract **meaningful hierarchical section headings** that define the structure of the document:\n",
        "     - H1: Top-level (e.g., Introduction, Abstract, Methodology)\n",
        "     - H2: Sub-sections (e.g., 1.1 Scope)\n",
        "     - H3: Sub-sub-sections (e.g., 1.1.1 Details)\n",
        "    - Some documents may not contain all levels (H1, H2, H3) — extract whatever is present.\n",
        "   - Do not assume a fixed font size threshold for each level — use layout and semantics to infer hierarchy.\n",
        "\n",
        "\n",
        "3. Do not classify something as a heading simply because it is:\n",
        "   - Bold\n",
        "   - Uppercase\n",
        "   - Indented or stylistically emphasized\n",
        "   These features are only weak signals. Headings must match structural importance semantically and visually.\n",
        "\n",
        "4. Special Case Clarification:\n",
        "   - If a document contains a \"Table of Contents\", **only the heading 'Table of Contents' itself is to be extracted** (as H1).\n",
        "   - The rest of the lines under it (even if numbered or bold) are **not actual headings**, but references — ignore them.\n",
        "   - Similarly, ignore section previews or index lists inside TOC or elsewhere.\n",
        "\n",
        "   - For “Table of Contents”, extract only the heading \"Table of Contents\" as heading — ignore all lines below it.\n",
        "   - Do **not** extract references from inside a TOC block.\n",
        "   - Recognize and include important headings like “Revision History” if present.\n",
        "\n",
        "\n",
        "5. Use semantic understanding to identify section boundaries:\n",
        "   - Don’t rely on font size alone.\n",
        "   - Some headings may be multiline — treat them as one unit.\n",
        "   - Use knowledge of common section names (e.g., Introduction, Abstract, References).\n",
        "\n",
        "6. Multilingual Handling:\n",
        "   - Support headings in other languages  — infer meaning contextually.\n",
        "   -Follow the same process for multilingual documents.\n",
        "   -Do not miss any headings or subheadings.\n",
        "   -Give more focus to visual features for multilingual document handling(if text is bold and big it is a heading)\n",
        "\n",
        "7. Ignore:\n",
        "   - Figure/table/image captions, footnotes, headers/footers, text inside tables or graphics.\n",
        "\n",
        "8. Output:\n",
        "   - Return only JSON as shown above for the entire PDF.\n",
        "   - Each heading object must include: { level: \"H1\"|\"H2\"|\"H3\", text: \"heading text\", page: page_number }\n",
        "   - Page number starts from 1.\n",
        "   -There is only one title for the entire document.It shouldnt appear for each page\n",
        "\n",
        "Be robust even on complex layouts or scanned text.\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khnVwnT81lNL"
      },
      "outputs": [],
      "source": [
        "def process_challenge_1a(pdf_path,parsinginstruct, lang):\n",
        "\n",
        "  withInstructionParsing=LlamaParse(api_key=LLAMA_CLOUD_API_KEY,result_type='markdown',parsing_instruction=parsinginstruct,language=lang).load_data(pdf_path)\n",
        "  n=len(withInstructionParsing)\n",
        "  return withInstructionParsing\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "un95b5EQ2Nz_"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def extract_outline_using_doc_index(markdown_docs, output_dir, pdf_filename):\n",
        "    all_headings = []\n",
        "    true_title = None\n",
        "\n",
        "    for idx, doc in enumerate(markdown_docs):\n",
        "        page_number = idx + 1  # Pages are 1-indexed\n",
        "\n",
        "        try:\n",
        "            parsed = json.loads(doc.text.strip())\n",
        "\n",
        "            # Only set the title once\n",
        "            title = parsed.get(\"title\", \"\").strip()\n",
        "            if not true_title and title:\n",
        "                true_title = title\n",
        "\n",
        "            # Extract outline entries but override the page with index-based page number\n",
        "            for item in parsed.get(\"outline\", []):\n",
        "                if not (item.get(\"level\") and item.get(\"text\")):\n",
        "                    continue\n",
        "\n",
        "                all_headings.append({\n",
        "                    \"level\": item[\"level\"],\n",
        "                    \"text\": item[\"text\"].strip(),\n",
        "                    \"page\": page_number\n",
        "                })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing markdown on page {page_number}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Deduplicate: keep latest occurrence by page\n",
        "    deduped = {}\n",
        "    for entry in all_headings:\n",
        "        key = (entry[\"level\"], entry[\"text\"])\n",
        "        if key not in deduped or entry[\"page\"] > deduped[key][\"page\"]:\n",
        "            deduped[key] = entry\n",
        "\n",
        "    # Sort by page number\n",
        "    sorted_headings = sorted(deduped.values(), key=lambda x: x[\"page\"])\n",
        "\n",
        "    final_json = {\n",
        "        \"title\": true_title or \"Untitled Document\",\n",
        "        \"outline\": sorted_headings\n",
        "    }\n",
        "\n",
        "    # Save to file\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Save to file using original PDF name (replace .pdf with .json)\n",
        "    json_filename = os.path.splitext(pdf_filename)[0] + \".json\"\n",
        "    output_path = os.path.join(output_dir, json_filename)\n",
        "\n",
        "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(final_json, f, ensure_ascii=False, indent=2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0EECxkf-hyW",
        "outputId": "f314c083-7683-4522-e3c0-fb1ceda86445"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.1/981.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m972.8/981.5 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pdf2image\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from pdf2image) (11.3.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (25.0)\n",
            "Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993332 sha256=3d0339e610d165fdae78a7e747faceab7291565cb4cacb245ab2a7a2f3b8b379\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
            "Successfully built langdetect\n",
            "Installing collected packages: pytesseract, pdf2image, langdetect\n",
            "Successfully installed langdetect-1.0.9 pdf2image-1.17.0 pytesseract-0.3.13\n"
          ]
        }
      ],
      "source": [
        "!pip install langdetect pdf2image pytesseract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqYXKTm2DdPR",
        "outputId": "d570dbc7-506f-4a9f-879b-86b71abfc55f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.3.tar.gz (73 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m71.7/73.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-3.0.0-py3-none-any.whl.metadata (10.0 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from fasttext) (80.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fasttext) (2.0.2)\n",
            "Using cached pybind11-3.0.0-py3-none-any.whl (292 kB)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.3-cp311-cp311-linux_x86_64.whl size=4508440 sha256=06813cdd70ae9358a77eaedb5700695d2cf285f5ceb48fc09ef79ead095aa9b7\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/4f/35/5057db0249224e9ab55a513fa6b79451473ceb7713017823c3\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.3 pybind11-3.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install fasttext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "ubOeP8f5Datf"
      },
      "outputs": [],
      "source": [
        "from pdf2image import convert_from_path\n",
        "import pytesseract\n",
        "import fasttext\n",
        "import tempfile\n",
        "import os\n",
        "\n",
        "# Pretrained language identification model (download if not already)\n",
        "FASTTEXT_MODEL_PATH = \"lid.176.ftz\"\n",
        "if not os.path.exists(FASTTEXT_MODEL_PATH):\n",
        "    import urllib.request\n",
        "    urllib.request.urlretrieve(\n",
        "        \"https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.ftz\",\n",
        "        FASTTEXT_MODEL_PATH\n",
        "    )\n",
        "\n",
        "# Load the model\n",
        "ft_model = fasttext.load_model(FASTTEXT_MODEL_PATH)\n",
        "\n",
        "# Lang → Tesseract mapping\n",
        "lang_map = {\n",
        "    \"hi\": \"hin\", \"ta\": \"tam\", \"fr\": \"fra\", \"en\": \"eng\", \"bn\": \"ben\",\n",
        "    \"mr\": \"mar\", \"gu\": \"guj\", \"te\": \"tel\", \"ur\": \"urd\", \"kn\": \"kan\",\n",
        "    \"ml\": \"mal\", \"pa\": \"pan\", \"or\": \"ori\", \"as\": \"asm\", \"ne\": \"nep\"\n",
        "}\n",
        "\n",
        "def detect_pdf_language(pdf_path, max_pages=2):\n",
        "    # Convert first few pages to images\n",
        "    images = convert_from_path(pdf_path, dpi=300, first_page=1, last_page=max_pages)\n",
        "\n",
        "    ocr_text = \"\"\n",
        "    for img in images:\n",
        "        ocr_text += pytesseract.image_to_string(img, lang='eng')  # fallback\n",
        "\n",
        "    # Run fasttext language detection\n",
        "    with tempfile.NamedTemporaryFile(mode=\"w+\", encoding=\"utf-8\", delete=False) as f:\n",
        "        f.write(ocr_text)\n",
        "        f.flush()\n",
        "        lang_pred = ft_model.predict(ocr_text.replace('\\n', ' '))[0][0].replace(\"__label__\", \"\")\n",
        "\n",
        "    # Map to Tesseract OCR language code\n",
        "    tesseract_lang = lang_map.get(lang_pred, \"eng\")\n",
        "\n",
        "    print(f\"🌐 Detected language: {lang_pred} → Using Tesseract lang: {tesseract_lang}\")\n",
        "\n",
        "    return tesseract_lang, lang_pred\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "ynVO8Jm8-nYg"
      },
      "outputs": [],
      "source": [
        "def run_ocr_in_detected_language(pdf_path):\n",
        "    tesseract_lang = detect_pdf_language(pdf_path)[0]\n",
        "\n",
        "    images = convert_from_path(pdf_path, dpi=300)\n",
        "    full_text = \"\"\n",
        "    for img in images:\n",
        "        full_text += pytesseract.image_to_string(img, lang=tesseract_lang)\n",
        "\n",
        "    return full_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "CLmV33mh_2DL"
      },
      "outputs": [],
      "source": [
        "lang_map = {\n",
        "    \"af\": \"afr\", \"ar\": \"ara\", \"bn\": \"ben\", \"zh-cn\": \"chi_sim\", \"zh-tw\": \"chi_tra\",\n",
        "    \"en\": \"eng\", \"fr\": \"fra\", \"de\": \"deu\", \"gu\": \"guj\", \"hi\": \"hin\", \"it\": \"ita\",\n",
        "    \"ja\": \"jpn\", \"kn\": \"kan\", \"ko\": \"kor\", \"ml\": \"mal\", \"mr\": \"mar\", \"ta\": \"tam\",\n",
        "    \"te\": \"tel\", \"ur\": \"urd\", \"pa\": \"pan\", \"es\": \"spa\", \"ru\": \"rus\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "nK0bBAD3_u2P"
      },
      "outputs": [],
      "source": [
        "import tempfile\n",
        "import os\n",
        "\n",
        "def save_to_markdown(text, pdf_path):\n",
        "    pdf_name = os.path.splitext(os.path.basename(pdf_path))[0]\n",
        "    md_path = os.path.join(tempfile.gettempdir(), pdf_name + \".md\")\n",
        "    with open(md_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(text)\n",
        "    return md_path\n",
        "\n",
        "\n",
        "# Step 4: Parse markdown using LlamaParse\n",
        "def parse_markdown_with_llamaparse(md_path, api_key):\n",
        "    parser = LlamaParse(api_key=api_key, result_type=\"markdown\")\n",
        "    docs = parser.load_data(md_path)\n",
        "    return docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWyqlKMMAgW_",
        "outputId": "3f8bf88f-f952-4866-96dd-1f83049fb7f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement poppler (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for poppler\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install poppler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cClvQmbwAmkY",
        "outputId": "c802787b-42ae-4fa8-e9d2-4300969f66a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement pdfinfo (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for pdfinfo\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install pdfinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "St0E3GNXA39a",
        "outputId": "143b2021-7333-4864-ff4d-4c5faee39072"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 697 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.8 [186 kB]\n",
            "Fetched 186 kB in 1s (302 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 126284 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.8_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.8) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.8) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ],
      "source": [
        "!apt-get install -y poppler-utils\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlsM5Hn3E27p",
        "outputId": "95ea15a9-4a08-4df3-a7b8-794bbfcbeb46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install numpy==1.26.4 --quiet\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvbJ18BZF3oh",
        "outputId": "7dae0451-53b1-4d14-9173-fc6b40fcb5f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m153.6/232.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "-tdqQHvzF9ry"
      },
      "outputs": [],
      "source": [
        "def detect_lang(file_path):\n",
        "  with open(file_path,'rb') as file:\n",
        "    pdf_reader=PyPDF2.PdfReader(file)\n",
        "    page=pdf_reader.pages[0]\n",
        "    text=page.extract_text()\n",
        "    lang=detect(text)\n",
        "    return lang"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_FSaTI52Pyy",
        "outputId": "b53651ff-347b-4f85-f5a8-1938b5627134"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING: parsing_instruction is deprecated. Use system_prompt, system_prompt_append or user_prompt instead.\n",
            "Started parsing the file under job_id a03898d4-84a4-4d52-a775-21051a661b6e\n",
            "Error parsing markdown on page 1: Expecting ',' delimiter: line 21 column 13 (char 261)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "from pdf2image import convert_from_path\n",
        "import pytesseract\n",
        "\n",
        "from langdetect import detect\n",
        "import PyPDF2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Path to the input directory containing PDF files\n",
        "input_dir = \"/content/input_dir\"\n",
        "output_dir=\"output_dir\"\n",
        "\n",
        "# List all PDF files in the directory\n",
        "pdf_files = [os.path.join(input_dir, f) for f in os.listdir(input_dir) if f.lower().endswith(\".pdf\")]\n",
        "\n",
        "# Now loop through them and process each\n",
        "for pdf_path in pdf_files:\n",
        "    filename = os.path.basename(pdf_path)\n",
        "    lang=detect_lang(pdf_path)\n",
        "    if lang in [\"en\",\"eng\",\"ENG\",\"ENGLISH\"]:\n",
        "      obj=process_challenge_1a(pdf_path,parsinginstruct,lang)\n",
        "      extract_outline_using_doc_index(obj,output_dir,filename)\n",
        "    else:\n",
        "      #ocr_text=run_ocr_in_detected_language(pdf_path)\n",
        "      #path=save_to_markdown(ocr_text, pdf_path)\n",
        "      obj=process_challenge_1a(pdf_path,parsinginstruct,lang)\n",
        "      extract_outline_using_doc_index(obj,output_dir,filename)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ZSHYECm95R6y"
      },
      "outputs": [],
      "source": [
        "!mkdir input_dir"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
